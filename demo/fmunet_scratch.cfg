[dataset]
# tensor type (float or double)
tensor_type = float

task_type = seg
train_dir = /home/disk4t/projects/PyMIC_project/PyMIC_examples/PyMIC_data/AtriaSeg/TrainingSet_crop
train_csv = demo/data/image_train.csv
valid_csv = demo/data/image_valid.csv
test_csv  = demo/data/image_test.csv

# modality number
modal_num = 1
train_batch_size = 2
valid_batch_size = 1
patch_size       = [64, 128, 128]

# data transforms
train_transform = [RandomCrop, NormalizeWithMeanStd, RandomFlip, LabelToProbability]
valid_transform = [NormalizeWithMeanStd, LabelToProbability]
test_transform  = [NormalizeWithMeanStd]

NormalizeWithMeanStd_channels =[0]

RandomCrop_foreground_focus = False
RandomCrop_foreground_ratio = None
RandomCrop_mask_label       = None
RandomCrop_inverse          = False

RandomFlip_flip_depth  = True
RandomFlip_flip_height = True
RandomFlip_flip_width  = True
RandomFlip_inverse     = False 

LabelToProbability_class_num = 2
LabelToProbability_inverse   = False

[network]
# this section gives parameters for network
# the keys may be different for different networks

net_type      = FMUNet
class_num     = 2
in_chns       = 1
feature_chns  = [32, 64, 128, 256, 512]
dropout       = 0.2
depth         = 2
res_mode      = 0
multiscale_pred = False


[training]
# list of gpus
gpus       = [0]

deep_supervise = False
loss_type     = [DiceLoss, CrossEntropyLoss]
loss_weight   = [0.5, 0.5]

# for optimizers
optimizer     = Adam
learning_rate = 1e-3
momentum      = 0.9
weight_decay  = 1e-5


# for lr schedular 
lr_scheduler = PolynomialLR
lr_power     = 0.8
early_stop_patience = 3000
ckpt_dir       = demo/model/fmunet_scratch

# start iter
iter_max   = 6000
iter_valid = 250
iter_save  = 6000

[testing]
# list of gpus
gpus       = [0]

ckpt_mode         = 1
output_dir        = demo/result/fmunet_scratch

sliding_window_enable = True
sliding_window_batch  = 8
sliding_window_size   = [64, 128, 128]
sliding_window_stride = [32, 64, 64]

